# 2.强化学习

## 1.RL基础

#### **1-1** QA: 用一句话谈一下你对于强化学习的认识吗？

强化学习包含环境、动作和奖励3部分，其本质是智能体通过与环境的交互，使其做出的动作对应的决策得到的总奖励最大，或者说是期望最大。

#### **1-2** QA: 请问，你认为强化学习、监督学习和无监督学习三者有什么区别呢？

首先强化学习和无监督学习是不需要有标签样本的，而监督学习需要许多有标签样本来进行模型的构建和训练。

其次对于强化学习与无监督学习，无监督学习直接基于给定的数据进行建模，寻找数据或特征中隐藏的结构，一般对应聚类问题；强化学习需要**通过延迟奖励学习策略来得到模型与目标的距离**，这个距离可以通过奖励函数进行定量判断，这里我们可以将奖励函数视为正确目标的一个稀疏、延迟形式。

另外，强化学习处理的**多是序列数据，样本之间通常具有强相关性**，但其很难像监督学习的样本一样满足独立同分布条件。

#### **1-3** QA: 根据你的理解，你认为强化学习的使用场景有哪些呢？

7个字总结就是“**多序列决策问题**”，或者说是对应的模型未知，需要通过学习逐渐逼近真实模型的问题。并且当前的动作会影响环境的状态，即具有马尔可夫性的问题。同时应满足所有状态是可重复到达的条件，即满足可学习条件。

#### **1-4** QA: 请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？

深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小，而强化学习中的损失函数的目的是**使总奖励的期望尽可能大**。

#### **1-5** QA: 你了解有模型和免模型吗？两者具体有什么区别呢？

我认为两者的区别主要在于**是否需要对真实的环境进行建模**，免模型方法不需要对环境进行建模，直接与真实环境进行交互即可，所以其通常需要较多的数据或者采样工作来优化策略，这也使其对于真实环境具有更好的泛化性能；而有模型方法需要对环境进行建模，同时在真实环境与虚拟环境中进行学习，如果建模的环境与真实环境的差异较大，那么会限制其泛化性能。现在通常使用有模型方法进行模型的构建工作。

## 2.马尔可夫决策过程

#### **2-1** QA：请问马尔可夫过程是什么？马尔可夫决策过程又是什么？其中马尔可夫最重要的性质是什么呢？

马尔可夫过程是一个二元组 $<S,P>$ ， $S$ 为状态集合， $P$ 为状态转移函数；

马尔可夫决策过程是一个五元组 $<S,P,A,R,\gamma>$， 其中 $R$ 表示从 $S$ 到 $S'$ 能够获得的奖励期望， $\gamma$ 为折扣因子， $A$ 为动作集合；

马尔可夫最重要的性质是**下一个状态只与当前状态有关，与之前的状态无关**，也就是 $p(s_{t+1} | s_t)= p(s_{t+1}|s_1,s_2,...,s_t)$。

#### **2-2** QA：请问我们一般怎么求解马尔可夫决策过程？

求解马尔可夫决策过程时，可以直接求解**贝尔曼方程或动态规划方程**：

$$
V(s)=R(S)+ \gamma \sum_{s' \in S}p(s'|s)V(s')
$$

特别地，其矩阵形式为 $\mathrm{V}=\mathrm{R}+\gamma \mathrm{PV}$。但是贝尔曼方程很难求解且计算复杂度较高，所以可以使用动**态规划、蒙特卡洛以及时序差分**等方法求解。

#### **2-3** QA：请问如果数据流不具备马尔可夫性质怎么办？应该如何处理？

如果不具备马尔可夫性，即下一个状态与之前的状态也有关，若仅用当前的状态来求解决策过程，势必导致决策的泛化能力变差。为了解决这个问题，可以**利用循环神经网络对历史信息建模，获得包含历史信息的状态表征**，表征过程也可以使用注意力机制等手段，最后在表征状态空间求解马尔可夫决策过程问题。

#### **2-4** QA：请分别写出基于状态价值函数的贝尔曼方程以及基于动作价值函数的贝尔曼方程。

贝尔曼方程：定义了当前状态与未来状态的迭代关系，表示当前状态的价值函数可以通过下个状态的价值函数来计算。贝尔曼方程即 $V(s)=R(s)+ \gamma \sum_{s' \in S}P(s'|s)V(s')$

1.  基于状态价值函数的贝尔曼方程：$V_{\pi}(s) = \sum_{a}{\pi(a|s)}\sum_{s',r}{p(s',r|s,a)[r(s,a)+\gamma V_{\pi}(s')]}$；
2.  基于动作价值函数的贝尔曼方程：$Q_{\pi}(s,a)=\sum_{s',r}p(s',r|s,a)[r(s',a)+\gamma V_{\pi}(s')]$。

#### **2-5** 计算贝尔曼方程的常见方法有哪些，它们有什么区别？

1.  **动态规划方法**（DP）：可用来计算价值函数的值。当**模型完全已知**时，使用贝尔曼方程，**迭代来计算**价值函数，并进行策略的改进。$v\left(S_{t}\right) \leftarrow \mathbb{E}_{\pi}\left[R_{t+1}+\gamma v\left(S_{t+1}\right)\right]$ 。举例：如果任务时预测从上海开车到北京所需的时间，动态规划是寻找几个**有经验的老司机（模型已知）**，在还没有出发时，统计每个老司机的预计到达时间，求平均值即可作为任务的估计值。
2.  **蒙特卡洛方法**（MC）：可用来计算价值函数的值。**无模型**方法，通过计算**所观察到样本的平均值**作为实际期望收益的近似。$v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(G_{t}-v\left(S_{t}\right)\right)$。以开车举例，现在找几个新司机，让他们开车从上海到北京，在北京，统计到北京所用的时间，取平均值作为任务的估计值。
3.  **时差学习**（TD）：为动态规划方法和蒙特卡洛方法的结合。**无模型**方法，它从每轮的经验数据中学习。TD学习可以从**不完整**的一轮数据中学习。$T D(0): v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma v\left(s_{t+1}\right)-v\left(S_{t}\right)\right)$。以开车举例，在出发时有个预估时间如20小时，现在新司机从上海出发，到达南京已经花费5个小时，南京到北京的预估时间为13小时，则上海到北京的预测时间可以使用13+5=18小时代替，即一部分真实值，一部分预测值。

#### **2-6** 马尔可夫奖励过程与马尔可夫决策过程的区别是什么？

**相对于马尔可夫奖励过程，马尔可夫决策过程多了一个决策过程**，其他的定义与马尔可夫奖励过程是类似的。由于多了一个决策，多了一个动作，因此状态转移也多了一个条件，即执行一个动作，导致未来状态的变化，其不仅依赖于当前的状态，也依赖于在当前状态下智能体采取的动作决定的状态变化。对于价值函数，它也多了一个条件，多了一个当前的动作，即当前状态以及采取的动作会决定当前可能得到的奖励的多少。

另外，**两者之间是有转换关系**的。具体来说，已知一个马尔可夫决策过程以及一个策略 $\pi$ 时，可以把马尔可夫决策过程转换成马尔可夫奖励过程。在马尔可夫决策过程中，状态的转移函数 $P(s'|s,a)$ 是基于它的当前状态和当前动作的，因为现在已知策略函数，即在每一个状态，知道其采取每一个动作的概率，所以我们就可以直接把这个动作进行加和，就可以得到对于马尔可夫奖励过程的一个转移概率。同样地，对于奖励，我们可以把动作去掉，这样就会得到一个类似于马尔可夫奖励过程的奖励。

#### **2-7** QA：请问最佳价值函数 $V^*$ 和最佳策略 $\pi^**$ 为什么等价呢？

最佳价值函数的定义为$V^* (s)=\max_{\pi} V_{\pi}(s)$，即我们搜索一种策略 $\pi$ 来让每个状态的价值最大。

$V^*$ 就是到达每一个状态其的最大价值，同时我们得到的策略就可以说是最佳策略，即 $\pi^{*}(s)=\underset{\pi}{\arg \max }~ V_{\pi}(s)$ 。最佳策略使得每个状态的价值函数都取得最大值。所以如果可以得到一个最佳价值函数，就可以说某一个马尔可夫决策过程的环境被解。在这种情况下，其最佳价值函数是一致的，即其达到的上限的值是一致的，但这里可能有多个最佳策略对应于相同的最佳价值。

#### **2-8** QA：能不能手写一下第n步的价值函数更新公式呀？另外，当 n越来越大时，价值函数的期望和方差是分别变大还是变小呢？

*n* 越大，方差越大，期望偏差越小。价值函数的更新公式如下：

$$
Q\left(S, A\right) \leftarrow Q\left(S, A\right)+\alpha\left[\sum_{i=1}^{n} \gamma^{i-1} r_{t+i}+\gamma^{n} \max _{a} Q\left(S',a\right)-Q\left(S, A\right)\right]
$$

## 3.表格型方法

#### **3-1** QA：同学，能否简述同策略和异策略的区别呢？

同策略和异策略的根本区别**在于生成样本的策略和参数更新时的策略是否相同**。

-   对于同策略，行为策略和要优化的策略是同一策略，更新了策略后，就用该策略的最新版本对数据进行采样；
-   对于异策略，其使用任意行为策略来对数据进行采样，并利用其更新目标策略。

例如，**Q学习**在计算下一状态的预期奖励时使用了最大化操作，直接选择最优动作，而当前策略并不一定能选择到最优的动作，因此这里生成样本的策略和学习时的策略不同，所以Q学习算法是**异策略算法**；相对应的**Sarsa**算法则是基于当前的策略直接执行一次动作选择，然后用动作和对应的状态更新当前的策略，因此生成样本的策略和学习时的策略相同，所以Sarsa算法为**同策略算法**。

#### **3-2** QA：能否细致地讲一下Q学习算法，最好可以写出其 $Q(s,a)$ 的更新公式。另外，它是同策略还是异策略，原因是什么呢？

Q学习是通过**计算最优动作价值函数来求策略的一种时序差分的学习方法**，其更新公式为

$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r(s,a) + \gamma \max_{a'} Q(s', a') - Q(s, a)]
$$

其是**异策略的**，由于Q更新使用了下一个时刻的最大值，因此其只关心哪个动作使得 $Q(s_{t+1}, a)$ 取得最大值，而实际上到底采取了哪个动作（行为策略），Q学习并不关心。这表明优化策略并没有用到行为策略的数据，所以说它是异策略的。

#### **3-3** QA：能否讲一下与Q学习算法类似的Sarsa算法呢，最好也可以写出其对应的 $Q(s,a)$ 更新公式。另外，它是同策略还是异策略，为什么？

Sarsa算法可以算是Q学习算法的改进，其更新公式为

$$
Q(s, a) \leftarrow Q(s, a) + \alpha [r(s,a) + \gamma  Q(s', a') - Q(s, a)]
$$

其为**同策略的**，Sarsa算法必须执行两次动作得到 $(s,a,r,s',a')$ 才可以更新一次；而且 $a'$ 是在特定策略 $\pi$ 的指导下执行的动作，因此估计出来的 $Q(s,a)$ 是在该策略 $\pi$ 下的Q值，样本生成用的 $\pi$ 和估计的 $\pi$ 是同一个，因此是同策略。

#### **3-4** QA：请问基于价值的方法和基于策略的方法的区别是什么？

1.  **生成策略上的差异，前者确定，后者随机**。基于价值的方法中动作-价值对的估计值最终会收敛（通常是不同的数，可以转化为0～1的概率），因此通常会获得一个确定的策略；基于策略的方法不会收敛到一个确定的值，另外他们会趋向于生成最佳随机策略。如果最佳策略是确定的，那么最优动作对应的值函数的值将远大于次优动作对应的值函数的值，值函数的大小代表概率的大小。
2.  **动作空间是否连续，前者离散，后者连续**。基于价值的方法，对于连续动作空间问题，虽然可以将动作空间离散化处理，但离散间距的选取不易确定。过大的离散间距会导致算法取不到最优动作，会在最优动作附近徘徊；过小的离散间距会使得动作的维度增大，会和高维度动作空间一样导致维度灾难，影响算法的速度。而基于策略的方法适用于连续的动作空间，在连续的动作空间中，可以不用计算每个动作的概率，而是通过正态分布选择动作。
3.  基于价值的方法，例如Q学习算法，是通过求解最优价值函数而间接地求解最优策略；基于策略的方法，例如REINFORCE等算法直接将策略参数化，通过策略搜索、策略梯度或者进化方法来更新参数以最大化回报。基于价值的方法不易扩展到连续动作空间，并且当同时采用非线性近似、自举等策略时会有收敛问题。策略梯度具有良好的收敛性。
4.  另外，对于价值迭代和策略迭代，策略迭代有两个循环，一个是在策略估计的时候，为了求当前策略的价值函数需要迭代很多次；另一个是外面的大循环，即策略评估、策略提升。价值迭代算法则是一步到位，直接估计最优价值函数，因此没有策略提升环节。

#### **3-5** QA：请简述一下时序差分方法。

时序差分算法是使用广义策略迭代来更新Q函数的方法，核心是使用自举，即**价值函数的更新使用下一个状态的价值函数来估计当前状态的价值**。也就是使用下一步的Q值 $Q(s_{t+1},a_{t+1})$ 来更新当前步的Q值 $Q(s_t,a_t)$。完整的计算公式如下：

$$
Q(s_t,a_t) \leftarrow   Q(s_t,a_t) + \alpha [r_{t+1}+\gamma Q(s_{t+1},a_{t+1})]
$$

#### **3-6** QA：请问蒙特卡洛方法和时序差分方法是无偏估计吗？另外谁的方差更大呢？为什么？

**蒙特卡洛方法是无偏估计，时序差分方法是有偏估计**；**蒙特卡洛方法的方差较大，时序差分方法的方差较小**，原因在于时序差分方法中使用了自举，实现了基于平滑的效果，导致估计的价值函数的方差更小。

#### **3-7** QA：能否简单说一下动态规划方法、蒙特卡洛方法和时序差分方法的异同点？

**相同点**：都用于进行价值函数的描述与更新，并且所有方法都基于对未来事件的展望计算一个回溯值。

**不同点**：蒙特卡洛方法和时序差分方法属于免模型方法，而动态规划属于有模型方法；时序差分方法和蒙特卡洛方法，因为都是免模型的方法，所以对于后续状态的获知也都是基于试验的方法；时序差分方法和动态规划方法的策略评估，都能基于当前状态的下一步预测情况来得到对于当前状态的价值函数的更新。

另外，时序差分方法不需要等到试验结束后才能进行当前状态的价值函数的计算与更新，而蒙特卡洛方法需要与环境交互，产生一整条马尔可夫链并直到最终状态才能进行更新。时序差分方法和动态规划方法的策略评估不同之处为免模型和有模型，动态规划方法可以凭借已知转移概率推断出后续的状态情况，而时序差分方法借助试验才能知道。

蒙特卡洛方法和时序差分方法的不同在于，蒙特卡洛方法进行了完整的采样来获取长期的回报值，因而在价值估计上会有更小的偏差，但是也正因为收集了完整的信息，所以价值的方差会更大，原因在于其基于试验的采样得到，和真实的分布有差距，不充足的交互导致较大方差。而时序差分方法则相反，因为它只考虑了前一步的回报值，其他都是基于之前的估计值，因而其价值估计相对来说具有偏差大方差小的特点。

三者的联系：对于TD($\lambda$)方法，如果 $\lambda = 0$ ，那么此时等价于时序差分方法，即只考虑下一个状态；如果 $\lambda = 1$ ，等价于蒙特卡洛方法，即考虑 $T-1$ 个后续状态直到整个试验结束。

## 4.DQN

#### **4-1** QA：请问深度Q网络是什么？其两个关键性的技巧分别是什么？

深度Q网络是基于深度学习的Q学习算法，其结合了**价值函数近似与神经网络技术**，并采用了**目标网络**和**经验回放技巧**进行网络的训练。

**Q函数（Q-function）**： 其也被称为动作价值函数（action-value function）。其输入是一个状态-动作对，即在某一具体的状态采取对应的动作，假设我们都使用某个策略 $\pi$ ，得到的累积奖励的期望值有多大。

#### **4-2** QA：深度Q网络中的两个技巧————目标网络和经验回放，其具体作用是什么呢？

在深度Q网络中某个动作价值函数的更新依赖于其他动作价值函数。**如果一直更新价值网络的参数，会导致更新目标不断变化，也就是在追逐一个不断变化的目标，这样势必会不太稳定**。为了解决基于时序差分的网络中，优化目标 $Q_{\pi}\left(s_{t}, a_{t}\right) =r_{t}+Q_{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 左右两侧会同时变化使得训练过程不稳定，从而增大回归难度的问题，目标网络选择将优化目标的右边即 $r_{t}+Q_{\pi}\left(s_{t+1}, \pi\left(s_{t+1}\right)\right)$ 固定，通过改变优化目标左边的网络参数进行回归。固定目标网络参数；梯度下降只更新策略网络参数；更新多次策略网络后，将策略网络参数复制到目标网络；

对于经验回放，其会构建一个回放缓冲区，用来保存许多数据，每一个数据的内容包括：状态 $s_t$、采取的动作 $a_t$、得到的奖励 $r_t$、下一个状态 $s_{t+1}$。使用 $\pi$ 与环境交互多次，把收集到的数据都放到回放缓冲区中。当回放缓冲区“装满”后，就会自动删去最早进入缓冲区的数据。在训练时，对于每一轮迭代都有相对应的批量（与我们训练普通网络一样，通过采样得到），然后用这个批量中的数据去更新Q函数。即Q函数**在采样和训练的时候会用到过去的经验数据，也可以消除样本之间的相关性**。

#### **4-3** QA：深度Q网络和Q学习有什么异同点？

整体来说，从名称就可以看出，两者的目标价值以及价值的更新方式基本相同。但有如下不同点：

1.  首先，深度Q网络将Q学习与深度学习结合，用深度网络来近似动作价值函数，而Q学习则是采用表格进行存储。
2.  深度Q网络采用了经验回放的技巧，从历史数据中随机采样，而Q学习直接采用下一个状态的数据进行学习。

#### **4-4** QA：请问，随机性策略和确定性策略有什么区别吗？

随机性策略表示为某个状态下动作取值的分布，确定性策略在每个状态只有一个确定的动作可以选。从熵的角度来说，确定性策略的熵为0，没有任何随机性。随机性策略有利于我们进行适度的探索，确定性策略不利于进行探索。

#### **4-5** QA：请问不打破数据相关性，神经网络的训练效果为什么就不好？

在神经网络中通常使用随机梯度下降法。随机的意思是随机选择一些样本来增量式地估计梯度，比如常用的批量训练方法。如果样本是相关的，就意味着前后两个批量很可能也是相关的，那么估计的梯度也会呈现出某种相关性。但是在极端条件下，后面的梯度估计可能会抵消掉前面的梯度估计量，从而使得训练难以收敛。

#### **4-6** QA：深度Q网络都有哪些变种？引入状态奖励的是哪种？

深度Q网络有5个经典的变种：双深度Q网络、竞争深度Q网络、优先级双深度Q网络、噪声网络、分布式Q函数。

1.  **双深度Q网络**（Double DQN）：**将动作选择和价值估计分开，避免Q值被过高估计**。在双深度Q网络中存在两个Q网络，第一个Q网络决定哪一个动作的Q值最大，从而决定对应的动作。另一方面，Q值是用 $Q'$ 计算得到的，这样就可以避免过度估计的问题。
2.  **竞争深度Q网络**（Dueing Network）：**将Q值分解为状态价值和优势函数，得到更多有用信息**。将原来的深度Q网络的计算过程分为两步。第一步计算一个与输入有关的标量 $\mathrm{V(s)}$；第二步计算一个向量 $\mathrm{A(s,a)}$ 对应每一个动作。最后的网络将两步的结果相加，得到我们最终需要的Q值。用一个公式表示就是 $\mathrm{Q(s,a)=V(s)+A(s,a)}$ 。
3.  **优先级双深度Q网络**（PER）：**将经验池中的经验按照优先级进行采样**。在使用经验回放时，均匀地取出回放缓冲区（reply buffer）中的采样数据，这里并没有考虑数据间的权重大小。但是应该将那些训练效果不好的数据对应的权重加大，即其应该有更大的概率被采样到。
4.  **噪声网络**（Noisy Net）：**Q函数中加入高斯噪声**。其在每一个回合开始的时候，即智能体要和环境交互的时候，在原来的Q函数的每一个参数上加上一个高斯噪声（Gaussian noise），把原来的Q函数变成 $\tilde{Q}$ ，即噪声Q函数。同样，我们把每一个网络的权重等参数都加上一个高斯噪声，就得到一个新的网络 $\tilde{Q}$ 。我们会使用这个新的网络与环境交互直到结束。
5.  **分布式Q函数**（Distribution Q-function）：对深度Q网络进行模型分布，将最终网络的输出的每一类别的动作再进行分布操作。Q函数代表累计期望，输出是一个期望，代表奖励，可能会丢失一些信息，分布式Q函数直接输出分布。
6.  **彩虹（rainbow）**：将7个技巧/算法综合起来的方法，7个技巧分别是——深度Q网络、双深度Q网络、优先级经验回放的双深度Q网络、竞争深度Q网络、异步优势演员-评论员算法（A3C）、分布式Q函数、噪声网络

#### **4-7** QA：请简述双深度Q网络原理。

深度Q网络由于**总是选择当前最优的动作价值函数来更新当前的动作价值函数，因此存在过估计问题**（估计的价值函数值大于真实的价值函数值）。为了解耦这两个过程，双深度Q网络使用两个价值网络，一个网络用来执行动作选择，然后用另一个网络的价值函数对应的动作值更新当前网络。

#### **4-8** QA：请问竞争深度Q网络模型有什么优势呢？

对于 $\boldsymbol{Q}(s,a)$ ，其对应的状态由于为表格的形式，因此是离散的，而实际的状态大多不是离散的。对于Q值 $\boldsymbol{Q}(s,a)=V(s)+\boldsymbol{A}(s,a)$ 。其中的 $V(s)$ 是对于不同的状态都有值， $\boldsymbol{A}(s,a)$ 对于不同的状态都有不同的动作对应的值。所以本质上，最终的矩阵 $\boldsymbol{Q}(s,a)$ 是将每一个 $V(s)$ 加到矩阵 $\boldsymbol{A}(s,a)$ 中得到的。但是有时我们更新时不一定会将 $V(s)$ 和 $\boldsymbol{Q}(s,a)$ 都更新。将其分成两个部分后，就不需要将所有的状态-动作对都采样一遍，可以使用更高效的估计Q值的方法将最终的 $\boldsymbol{Q}(s,a)$ 计算出来。

## 5.策略梯度

#### **5-1** QA：如何理解策略梯度的公式呢？

策略梯度的公式如下：

$$
\begin{aligned}
E_{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right] &\approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(\tau^{n}\right) \\
&=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
\end{aligned}
$$

$p_{\theta}(\tau)$ 里面有两项，$p(s_{t+1}|s_t,a_t)$ 来自环境，$p_\theta(a_t|s_t)$ 来自智能体。 $p(s_{t+1}|s_t,a_t)$ 由环境决定，从而与 $\theta$ 无关，因此 $\nabla \log p(s_{t+1}|s_t,a_t) =0$ ， $\nabla p_{\theta}(\tau)=\nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)$。

具体来说：

（1）假设在状态 $s_t$ 时执行动作 $a_t$，最后发现轨迹 $\tau$ 的奖励是正的，那我们就要增大这一项的概率，即增大在状态 $s_t$ 时执行动作 $a_t$ 的概率；

（2）反之，在状态 $s_t$ 时执行动作 $a_t$ 会导致轨迹 $\tau$ 的奖励变成负的，我们就要减小这一项的概率。

#### **5-2** QA：同学来吧，给我手动推导一下策略梯度公式的计算过程。

首先我们的目的是最大化奖励函数，即调整 $\theta$ ，使得期望回报最大，可以用公式表示如下：

$$
J(\theta)=E_{\tau \sim p_{\theta(\tau)}}\left[\sum_tr(s_t,a_t)\right]
$$

其中 $\tau$ 表示从开始到结束的一条完整轨迹。通常对于最大化问题，我们可以使用梯度上升算法找到最大值，即

$$
\theta^* = \theta + \alpha\nabla J({\theta})
$$

所以仅仅需要计算并更新 $\nabla J({\theta})$ ，也就是计算奖励函数 $J({\theta})$ 关于 $\theta$ 的梯度，也就是策略梯度，计算方法如下：

$$
\nabla_{\theta}J(\theta) = \int {\nabla}_{\theta}p_{\theta}(\tau)r(\tau) \mathrm{d}{\tau}=\int p_{\theta}{\nabla}_{\theta} \mathrm{log}p_{\theta}(\tau)r(\tau)\mathrm{d}{\tau}=E_{\tau \sim p_{\theta}(\tau)}[{\nabla}_{\theta}\mathrm{log}p_{\theta}(\tau)r(\tau)]
$$

接着我们继续展开，对于 $p_{\theta}(\tau)$ ，即 $p_{\theta}(\tau|{\theta})$ ：

$$
p_{\theta}(\tau|{\theta}) = p(s_1)\prod_{t=1}^T \pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)
$$

取对数后为：

$$
\mathrm{log}p_{\theta}(\tau|{\theta}) = \mathrm{log}p(s_1)+\sum_{t=1}^T \mathrm{log}\pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)
$$

继续求导：

$$
\nabla \mathrm{log}p_{\theta}(\tau|{\theta}) = \sum_{t=1}^T \nabla_{\theta}\mathrm{log} \pi_{\theta}(a_t|s_t)
$$

代入第3个式子，可以将其化简为：

$$
\begin{aligned}
    \nabla_{\theta}J(\theta) 
    &= E_{\tau \sim p_{\theta}(\tau)}[{\nabla}_{\theta}\mathrm{log}p_{\theta}(\tau)r(\tau)] \\
    &= E_{\tau \sim p_{\theta}}[(\nabla_{\theta}\mathrm{log}\pi_{\theta}(a_t|s_t))(\sum_{t=1}^Tr(s_t,a_t))] \\
    &= \frac{1}{N}\sum_{i=1}^N[(\sum_{t=1}^T\nabla_{\theta}\mathrm{log} \pi_{\theta}(a_{i,t}|s_{i,t}))(\sum_{t=1}^Nr(s_{i,t},a_{i,t}))]    
\end{aligned}
$$

#### **5-3** QA：可以说一下你所了解的基于策略梯度优化的技巧吗？

（1）**增加基线**：为了防止所有奖励都为正，从而导致每一个状态和动作的变换，都会使得每一个变换的概率上升，把奖励减去一项 $b$，称 $b$ 为基线。当减去 $b$ 以后，就可以让奖励 $R(\tau^n)-b$ 有正有负。如果得到的总奖励 $R(\tau^n)$ 大于 $b$ ，就让它的概率上升。如果总奖励小于 $b$，就算它是正的，值很小也是不好的，就需要让它的概率下降。如果总奖励小于 $b$ ，就要让采取这个动作的奖励下降，这样也符合常理。但是使用基线会让本来奖励很大的“动作”的奖励变小，降低更新速率。

（2）**指派合适的分数**：首先，原始权重是整个回合的总奖励。现在改成从某个时间点 $t$ 开始，假设这个动作是在时间点 $t$ 被执行的，那么从时间点 $t$ ，一直到游戏结束所有奖励的总和，才真的代表这个动作是好的还是不好的；接下来我们再进一步，把未来的奖励打一个折扣，这里我们称由此得到的奖励的和为折扣回报。

（3）综合以上两种技巧，将其统称为**优势函数**，用 $A$ 来代表优势函数。优势函数取决于状态和动作，即我们需计算的是在某一个状态 $s$ 采取某一个动作 $a$ 的时候，优势函数有多大。

#### **5-4** QA：请详细描述REINFORCE算法的计算过程。

首先需要根据一个确定好的策略模型来输出每一个可能动作的概率，对于所有动作的概率，使用采样方法（或者是随机的方法）选择一个动作与环境进行交互，同时环境会给我们反馈整个回合的数据。将此回合数据输入学习函数中，并根据回合数据进行损失函数的构造，通过Adam等优化器的优化，再更新策略模型。

## 6.演员-评论家算法

#### **6-1** QA：请简述一下异步优势演员-评论员算法（A3C），另外A3C是同策略还是异策略的模型呀？

A3C是异步优势演员-评论员算法，其中，\*\*评论员学习价值函数，同时有多个演员并行训练并且不时与全局参数同步。A3C旨在并行训练，是同策略算法。 \*\*

#### **6-2** QA：请问演员-评论员算法有何优点呢？

1.  相比以价值函数为中心的算法，**演员-评论员算法应用了策略梯度的技巧**，这能让它在连续动作或者高维动作空间中选取合适的动作，而Q学习做这件事会很困难。
2.  相比单纯策略梯度，**演员-评论员算法应用了Q学习或其他策略评估的做法**，使得演员-评论员算法能进行单步更新而不是回合更新，比单纯的策略梯度的效率要高。

#### **6-3** QA：请问异步优势演员-评论员算法具体是如何异步更新的？

下面是异步优势演员-评论员算法的大纲，由于其为异步多线程算法，只对其中某一单线程进行分析。

（1）定义全局参数 $\theta$ 和 $w$ 以及特定线程参数 $\theta'$ 和 $w'$。

（2）初始化时间步 $t=1$。

（3）当 $T \leqslant T_{\mathrm{max}}$:

-   重置梯度：$\mathrm{d} \theta = 0$ 并且 $\mathrm{d}w = 0$。
-   将特定于线程的参数与全局参数同步：$\theta' = \theta$ 以及 $w'=w$。
-   令 $t_{\mathrm{start}} =t$ 并且随机采样一个初始状态 $s_t$。
-   当 （$s_t!=$ 终止状态）并且$t−t_{\mathrm{start}} \leqslant t_{\mathrm{max}}$。
    -   根据当前线程的策略选择当前执行的动作 $a_t\sim\pi_{\theta'}(a_t|s_t)$，执行动作后接收奖励 $r_t$ 然后转移到下一个状态 $s_{t+1}$。
    -   更新 $t$ 以及 $T$：$t=t+1$ 并且 $T=T+1$。
-   初始化保存累积奖励估计值的变量。
-   对于 $i=t_1, \dots ,t_{\mathrm{start}}$:
    -   $r \gets \gamma r+r_i$；这里的 $r$ 是 $G_i$ 的蒙特卡洛估计。
    -   累积关于参数 $\theta'$ 的梯度：$\mathrm{d} \theta \gets \mathrm{d}\theta + \nabla_{\theta'} \mathrm{log} \pi_{\theta'}(a_i|s_i)(r−V_{w'}(s_i))$。
    -   累积关于参数 $w'$ 的梯度：$\mathrm{d}w \gets \mathrm{d}w+ \mathrm{\partial} (r-V_{w'}(s_i))^2 / \mathrm{\partial} w'$。
-   分别使用 $\mathrm{d}\theta$ 以及 $\mathrm{d}w$ 异步更新 $\theta$ 以及 $w$。

#### **6-4** QA：演员-评论员算法中，演员和评论员两者的区别是什么？

演员是策略模块，输出动作；

评论员是判别器，用来计算价值函数。

#### **6-5** QA：演员-评论员算法框架中的评论员起了什么作用？

**评论员衡量当前决策的好坏**。结合策略模块，当评论员判别某个动作的选择是有益的时候，策略就更新参数以增大该动作出现的概率，反之减小该动作出现的概率。

#### **6-6** QA：简述异步优势演员-评论员算法的优势函数。

优势函数的计算公式为 $A(s,a)=Q(s,a)-V(s)=r+\gamma V(s')-V(s)$ ，其可以定量地表示选择动作 $a$ 的优势。即当动作 $a$ 低于价值函数的平均值的时候，优势函数为负值；反之为正值。其是一个标量，具体来说：

-   如果 $A(s,a)>0$ ，梯度被推向正方向；
-   如果 $A(s,a)<0$ ，即我们的动作比该状态下的平均值还差，则梯度被推向反方向。

这样就需要两个价值函数，所以可以使用时序差分方法做误差估计：$A(s,a)=r+\gamma V(s')-V(s)$ 。

## 7.DDPG算法

#### **7-1** QA：请简述一下深度确定性策略梯度算法。

在连续控制领域经典的强化学习算法，是深度Q网络在处理连续动作空间的一个扩充方法。

深度确定性策略梯度算法使用**演员-评论员结构**，但是输出的不是动作的概率，而是具体动作，其可以用于连续动作的预测。优化的目的是将深度Q网络扩展到连续的动作空间。另外，其含义如其名：

（1）深度是因为用了深度神经网络；

（2）确定性表示其输出的是一个确定的动作，可以用于连续动作的环境；

（3）策略梯度代表的是它用到的是策略网络。强化算法每个回合就会更新一次网络，但是深度确定性策略梯度算法每个步骤都会更新一次策略网络，它是一个单步更新的策略网络。

#### **7-2** QA：请问深度确定性策略梯度算法是同策略算法还是异策略算法？请说明具体原因并分析。

异策略算法。

1.  深度确定性策略梯度算法是优化的深度Q网络，其使用了经验回放，所以为异策略算法。
2.  因为深度确定性策略梯度算法为了保证一定的探索，对输出动作加了一定的噪声，行为策略不再是优化的策略。

#### **7-3** QA：你是否了解过分布的分布式深度确定性策略梯度算法（distributed distributional deep deterministic policy gradient，D4PG）呢？请描述一下吧。

分布的分布式深度确定性策略梯度算法（distributed distributional deep deterministic policy gradient，D4PG)，相对于深度确定性策略梯度算法，其优化部分如下。&#x20;

（1）分布式评论员：不再只估计Q值的期望值，而是估计期望Q值的分布，即将期望Q值作为一个随机变量来估计。

（2）$N$步累计回报：计算时序差分误差时，D4PG计算的是$N$步的时序差分目标值而不仅仅只有一步，这样就可以考虑未来更多步骤的回报。

（3）多个分布式并行演员：D4PG使用$K$个独立的演员并行收集训练数据并存储到同一个回放缓冲区中。

（4）优先经验回放（prioritized experience replay，PER）：使用一个非均匀概率从回放缓冲区中进行数据采样。

## 8.PPO算法

#### **8-1** QA：请问什么是重要性采样呀？

使用另外一种分布，来逼近所求分布的一种方法，算是一种期望修正的方法，公式如下：

$$
\int f(x) p(x) \mathrm{d} x=\int f(x) \frac{p(x)}{q(x)} q(x) \mathrm{d} x=E_{x \sim q}[f(x){\frac{p(x)}{q(x)}}]=E_{x \sim p}[f(x)]
$$

在已知 $q$ 的分布后，可以使用上式计算出从 $p$ 分布的期望值。也就可以使用 $q$ 来对 $p$ 进行采样了，即重要性采样。

#### **8-2** QA：请问同策略和异策略的区别是什么？

可以用一句话概括两者的区别，即**生成样本的策略（价值函数）和网络参数更新时的策略（价值函数）是否相同**。

-   **同策略（on-policy）**：要学习的智能体和与环境交互的智能体是同一个时对应的策略。
-   **异策略（off-policy）**：要学习的智能体和与环境交互的智能体不是同一个时对应的策略。

#### 8.3 QA：近端策略优化（proximal policy optimization，PPO）

避免在使用重要性采样时由于在 $\theta$ 下的 $p_{\theta}\left(a_{t} | s_{t}\right)$ 与在  $\theta '$ 下的 $p_{\theta'}\left(a_{t} | s_{t}\right)$ 相差太多，导致重要性采样结果偏差较大而采取的算法。具体来说就是在训练的过程中增加一个限制，这个限制对应 $\theta$ 和 $\theta'$ 输出的动作的KL散度，来衡量 $\theta$ 与 $\theta'$ 的相似程度。

#### **8-4** QA：请简述一下近端策略优化算法。其与信任区域策略优化算法有何关系呢？

近端策略优化算法借鉴了信任区域策略优化算法，通过采用一阶优化，在采样效率、算法表现以及实现和调试的复杂度之间取得了新的平衡。这是因为近端策略优化算法会在每一次迭代中尝试计算新的策略，让损失函数最小化，并且保证每一次新计算出的策略能够和原策略相差不大。换句话说，其为在避免使用重要性采样时由于在 $\theta$ 下的 $p_{\theta}\left(a_{t} | s_{t}\right)$ 与在 $\theta'$ 下的 $p_{\theta'}\left(a_{t} | s_{t}\right)$ 差太多，导致重要性采样结果偏差较大而采取的算法。

## 9.稀疏奖励

#### **9-1** QA：解决稀疏奖励的方法有哪些？

-   **设计奖励（reward shaping）**：当智能体与环境进行交互时，人为设计一些奖励，从而“指挥”智能体，告诉其采取哪一个动作是最优的。需要注意的是，这个奖励区别于环境的奖励。其可以提高我们估算Q函数时的准确性。
-   **内在好奇心模块（intrinsic curiosity module，ICM）**：其代表好奇心驱动这个技术中的增加新的奖励函数以后的奖励函数。
-   **课程学习（curriculum learning）**：一种广义的用在强化学习中训练智能体的方法，其在输入训练数据的时候，采取由易到难的顺序进行输入，也可以人为设计它的学习过程。这个方法在机器学习和强化学习中普遍使用。
-   **逆课程学习（reverse curriculum learning）**：相较于课程学习，逆课程学习为更广义的方法。其从最终最理想的状态 \[我们称之为黄金状态（gold state）] 开始，依次去寻找距离黄金状态最近的状态作为想让智能体达到的阶段性的“理想”状态。当然，会在此过程中有意地去掉一些极端的状态，即太简单、太难的状态。综上，逆课程学习是从黄金状态反推的方法。
-   **分层强化学习（hierarchical reinforcement learning）**：将一个大型的任务，横向或者纵向地拆解成由多个智能体去执行的子任务。其中，有一些智能体负责比较高层次的任务，如负责定目标，定完目标后，再将目标分配给其他的智能体执行。

#### **9-2** QA：设计奖励存在什么主要问题？

主要的问题是**人为设计的奖励需要领域知识**，需要自己设计出让环境与智能体更好地交互的奖励，这需要不少的经验知识，并且需要我们根据实际的效果进行调整。

#### **9-3** QA：内在好奇心模块是什么？我们应该如何设计内在好奇心模块？

内在好奇心模块代表好奇心驱动技术中增加新的奖励函数以后的奖励函数。具体来说，其在更新计算时会考虑3个新的部分，分别是状态 $s_1$、动作 $a_1$ 和状态 $s_2$。根据 $s_1$ 、$a_1$、$a_2$，它会输出另外一个新的奖励 $r_1^i$。所以在内在好奇心模块中，我们的总奖励并不是只有 $r$ 而已，还有 $r^i$。它不是只把所有的 $r$ 相加，还把所有 $r^i$ 相加一并当作总奖励。所以，基于内在好奇心模块的智能体在与环境交互的时候，不是只希望 $r$ 越大越好，还同时希望 $r^i$ 越大越好，希望从内在好奇心模块里面得到的总奖励越大越好。

对于如何设计内在好奇心模块，其输入就像前面所说的一样，包括3部分，即现在的状态 $s_1$、在这个状态采取的动作 $a_1$、下一个状态 $s_{t+1}$，对应的输出就是奖励 $r_1^i$。输入、输出的映射是通过网络构建的，其使用状态 $s_1$ 和动作 $a_1$ 去预测下一个状态 $\hat{s}_{t+1}$ ，然后继续评判预测的状态 $\hat{s}_{t+1}$ 和真实状态 $s_{t+1}$ 的相似性，越不相似得到的奖励就越大。通俗来说这个奖励就是，如果未来的状态越难被预测，那么得到的奖励就越大。这就是好奇心机制，其倾向于让智能体做一些风险比较大的动作，从而提高其探索的能力。

同时，为了进一步增强网络的表达能力，我们通常将内在好奇心模块的输入优化为特征提取，特征提取器的输入就是状态，输出是一个特征向量，其可以表示这个状态最主要和最重要的特征，把没有意义的事物过滤。

## 10.模仿学习

#### **10-1** QA：具体的模仿学习方法有哪些？

行为克隆、逆强化学习或者称为逆最优控制。

-   **行为克隆（behavior cloning）**：类似于机器学习中的监督学习，通过收集专家的状态与动作等对应信息，来训练我们的网络。在使用时，输入状态就可以输出对应的动作。
-   **数据集聚合（dataset aggregation）**：用来应对在行为克隆中专家提供不到数据的情况，其希望收集专家在各种极端状态下的动作。
-   **逆强化学习（inverse reinforcement learning，IRL）**：逆强化学习先找出奖励函数，再用强化学习找出最优演员。这么做是因为我们没有环境中的奖励，但是有专家的示范，使用逆强化学习，我们可以推断专家是因为何种奖励函数才会采取这些动作。有了奖励函数以后就可以使用一般的强化学习方法找出最优演员。

#### **10-2** QA：行为克隆存在哪些问题呢？对应的解决方法有哪些？

（1）首先，如果只收集专家的示范（看到某一个状态输出的动作），那么**所有的结果会是非常有限的**。所以要收集专家在各种极端状态下的动作或者说要收集更多、更复杂的数据，可以使用数据集聚合方法。

（2）另外，使用传统意义上的行为克隆，**智能体会完全复制专家的行为，不管专家的行为是否合理**，智能体都会硬把它记下来。智能体是一个网络，网络的容量是有限的。就算给网络足够的训练数据，它在训练数据集上得到的正确率往往也不是100\\%。所以这个时候，什么该学、什么不该学就变得很重要。实际上，极少数专家的行为是没有意义的，但是使用它们的示范至少不会产生较坏的影响。

（3）还有，**在进行行为克隆的时候，训练数据和测试数据往往是不匹配的**。可以用数据集聚合来缓解这个问题。具体来说，在训练和测试的时候，数据分布是不同的。因为在强化学习中，动作会影响到接下来的状态。我们先有状态 $s_1$ ，然后采取动作 $a_1$ ，动作 $a_1$ 会决定接下来的状态 $s_2$ 。如果 $\pi^*$ 与 $\hat{\pi}$ 一模一样，那么我们训练时看到的状态与测试时看到的状态会是一样的，这样模型的泛化性能就会变得比较差。而且， $\pi^*$ 和 $\hat{\pi}$ 可能有一点儿误差，虽然这个误差在监督学习中，由于每一个样本都是独立的，因此影响不大，但对强化学习来说，可能在某个地方，也许智能体无法完全复制专家的行为，最后得到的结果就会差很多。所以行为克隆并不能够完全解决模仿学习的问题，我们可以使用另外一个比较好的方法，即逆强化学习。

#### **10-3** QA：逆强化学习是怎么运行的呢？

首先，有一个专家，其策略为 $\hat{\pi}$，这个专家负责与环境交互，给我们 $\hat{\tau_1}$ ～ $\hat{\tau_n}$，需要将其中的状态-动作序列都记录下来。然后对于演员，其策略为$\pi$，也需要进行一样的交互和序列的记录。接着需要指定一个奖励函数，并且保证专家对应的分数一定要比演员的要高，用这个奖励函数继续学习并更新我们的训练，同时套用一般条件下的强化学习方法进行演员网络的更新。在这个过程中，也要同时进行一开始指定的奖励函数的更新，使得演员得分越来越高，但是不超过专家的得分。最终的奖励函数应该让专家和演员对应的奖励函数都达到比较高的分数，并且从最终的奖励函数中无法分辨出两者。

#### **10-4** QA：逆强化学习方法与生成对抗网络在图像生成中有什么异曲同工之处？

在生成对抗网络中，有一些比较好的图片数据集，也有一个生成器，一开始其不知道要生成什么样的图片，只能随机生成。另外，我们有一个判别器，其用来给生成的图片打分，专家生成的图片得分高，生成器生成的图片得分低。有了判别器以后，生成器会想办法去“骗”判别器。生成器希望判别器也给它生成的图片打高分。整个过程与逆强化学习的过程是类似的。

（1）生成的图片就是专家的判别结果，生成器就是演员，生成器会生成很多的图片并让演员与环境进行交互，从而产生很多轨迹。这些轨迹与环境交互的记录等价于生成对抗网络中的生成图片。

（2）逆强化学习中的奖励函数就是判别器。奖励函数给专家的实例打高分，给演员的交互结果打低分。

（3）考虑两者的过程，在逆强化学习中，演员会想办法从已经学习到的奖励函数中获得高分，然后迭代地循环。这个过程其实是与生成对抗网络的训练过程一致的。
